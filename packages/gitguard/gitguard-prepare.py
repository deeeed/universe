#!/usr/bin/env python3
"""GitGuard - A tool to help maintain consistent git commit messages."""

import sys
import os
from pathlib import Path
from subprocess import check_output
import json
from typing import Dict, List, Optional, Set, Any
import requests
from collections import defaultdict
import re

# Try to import optional dependencies / check
try:
    from openai import AzureOpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    import tiktoken
    HAS_TIKTOKEN = True
except ImportError:
    HAS_TIKTOKEN = False

class Config:
    """Configuration handler with global and local settings."""
    DEFAULT_CONFIG = {
        "auto_mode": False,
        "use_ai": True,
        "ai_provider": "azure",  # Can be 'azure' or 'ollama'
        "azure_endpoint": "https://your-endpoint.openai.azure.com/",
        "azure_deployment": "gpt-4",
        "azure_api_version": "2024-05-13",
        "ollama_host": "http://localhost:11434",
        "ollama_model": "codellama",
        "debug": False,
    }

    def __init__(self):
        self._config = self.DEFAULT_CONFIG.copy()
        self._load_configurations()

    def _load_json_file(self, path: Path) -> Dict:
        try:
            if path.exists():
                return json.loads(path.read_text())
        except Exception as e:
            if self._config.get("debug"):
                print(f"‚ö†Ô∏è  Error loading config from {path}: {e}")
        return {}

    def _load_configurations(self):
        # 1. Global configuration
        global_config = self._load_json_file(Path.home() / ".gitguard" / "config.json")
        self._config.update(global_config)

        # 2. Local configuration
        try:
            git_root = Path(check_output(["git", "rev-parse", "--show-toplevel"], text=True).strip())
            local_config = self._load_json_file(git_root / ".gitguard" / "config.json")
            self._config.update(local_config)
        except Exception:
            pass

        # 3. Environment variables
        env_mappings = {
            "GITGUARD_AUTO": ("auto_mode", lambda x: x.lower() in ("1", "true", "yes")),
            "GITGUARD_USE_AI": ("use_ai", lambda x: x.lower() in ("1", "true", "yes")),
            "AZURE_OPENAI_ENDPOINT": ("azure_endpoint", str),
            "AZURE_OPENAI_DEPLOYMENT": ("azure_deployment", str),
            "AZURE_OPENAI_API_VERSION": ("azure_api_version", str),
            "GITGUARD_DEBUG": ("debug", lambda x: x.lower() in ("1", "true", "yes")),
        }

        for env_var, (config_key, transform) in env_mappings.items():
            if (value := os.environ.get(env_var)) is not None:
                self._config[config_key] = transform(value)

        if self._config.get("debug"):
            print("\nüîß Active configuration:", json.dumps(self._config, indent=2))

    def get(self, key: str, default=None):
        return self._config.get(key, default)

class OllamaClient:
    """Client for interacting with Ollama API."""
    def __init__(self, host: str, model: str):
        self.host = host.rstrip("/")
        self.model = model

    def generate(self, prompt: str, original_message: str) -> Optional[List[Dict[str, str]]]:
        """Generate commit message suggestions using Ollama."""
        try:
            response = requests.post(
                f"{self.host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                },
            )
            response.raise_for_status()
            
            result = response.json()
            response_text = result.get("response", "")
            
            try:
                # Find JSON object in the response
                start = response_text.find("{")
                end = response_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_str = response_text[start:end]
                    suggestions = json.loads(json_str).get("suggestions", [])
                    return suggestions[:3]
            except json.JSONDecodeError:
                print("\n‚ö†Ô∏è  Failed to parse Ollama response as JSON")
            
            # Fallback: Create a single suggestion from the raw response
            return [{
                "message": response_text.split("\n")[0],
                "explanation": "Generated by Ollama",
                "type": "feat",
                "scope": "default",
                "description": response_text,
            }]
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Ollama API error: {str(e)}")
            return None

def debug_log(message: str, title: str = "Debug", separator: bool = True) -> None:
    """Print debug messages in a clearly visible format."""
    if not Config().get("debug"):
        return
        
    print("\n" + "‚ïê" * 80)
    print(f"üîç {title.upper()}")
    print("‚ïê" * 80)
    print(message)
    if separator:
        print("‚ïê" * 80)

def calculate_commit_complexity(packages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Calculate commit complexity metrics to determine if structured format is needed."""
    complexity = {
        "score": 0,
        "reasons": [],
        "needs_structure": False
    }
    
    # 1. Multiple packages changes (most significant factor)
    if len(packages) > 1:
        complexity["score"] += 3
        complexity["reasons"].append("Changes span multiple packages")
    
    # 2. Number of files changed
    total_files = sum(len(pkg["files"]) for pkg in packages)
    if total_files > 3:
        complexity["score"] += min(total_files - 3, 5)  # Cap at 5 points
        complexity["reasons"].append(f"Large number of files changed ({total_files})")
    
    # 3. Mixed content types (e.g., code + tests + config)
    content_types = set()
    for pkg in packages:
        for file in pkg["files"]:
            if file.endswith(('.test.ts', '.test.js', '.spec.ts', '.spec.js')):
                content_types.add('test')
            elif file.endswith(('.json', '.yml', '.yaml', '.config.js')):
                content_types.add('config')
            elif file.endswith(('.css', '.scss', '.less')):
                content_types.add('styles')
            elif file.endswith(('.ts', '.js', '.tsx', '.jsx')):
                content_types.add('code')
            
    if len(content_types) > 2:
        complexity["score"] += 2
        complexity["reasons"].append("Multiple content types modified")
    
    # Determine if structured commit is needed (threshold = 5)
    complexity["needs_structure"] = complexity["score"] >= 5
    
    if Config().get("debug"):
        debug_log(
            f"Score: {complexity['score']}\nReasons:\n" + 
            "\n".join(f"- {reason}" for reason in complexity["reasons"]),
            "Complexity Analysis üìä"
        )
    
    return complexity

def group_files_by_type(files: List[str]) -> Dict[str, List[str]]:
    """Group files by their type for better readability."""
    groups = {
        "Tests": [],
        "Config": [],
        "Styles": [],
        "Source": []
    }
    
    for file in files:
        if file.endswith(('.test.ts', '.test.js', '.spec.ts', '.spec.js')):
            groups["Tests"].append(file)
        elif file.endswith(('.json', '.yml', '.yaml', '.config.js')):
            groups["Config"].append(file)
        elif file.endswith(('.css', '.scss', '.less')):
            groups["Styles"].append(file)
        else:
            groups["Source"].append(file)
    
    # Return only non-empty groups
    return {k: v for k, v in groups.items() if v}

def enhance_ai_prompt(packages: List[Dict], original_msg: str) -> str:
    """Generate detailed AI prompt based on commit complexity analysis."""
    complexity = calculate_commit_complexity(packages)
    
    try:
        diff = check_output(["git", "diff", "--cached"]).decode("utf-8")
    except:
        diff = "Failed to get diff"
    
    # Build comprehensive analysis for AI
    analysis = {
        "complexity_score": complexity["score"],
        "complexity_reasons": complexity["reasons"],
        "packages": []
    }
    
    for pkg in packages:
        files_by_type = group_files_by_type(pkg["files"])
        analysis["packages"].append({
            "name": pkg["name"],
            "scope": pkg["scope"],
            "files_by_type": files_by_type
        })

    prompt = f"""Analyze the following git changes and suggest a commit message.

Complexity Analysis:
- Score: {complexity['score']} (threshold for structured format: 5)
- Factors: {', '.join(complexity['reasons'])}

Changed Packages:"""

    for pkg in analysis["packages"]:
        prompt += f"\n\nüì¶ {pkg['name']} ({pkg['scope']})"
        for file_type, files in pkg["files_by_type"].items():
            prompt += f"\n{file_type}:"
            for file in files:
                prompt += f"\n  - {file}"

    prompt += f"""

Original message: "{original_msg}"

Git diff:
```diff
{diff}
```

Please provide 3 conventional commit suggestions in this JSON format:
{{
    "suggestions": [
        {{
            "message": "complete commit message",
            "explanation": "reasoning",
            "type": "commit type",
            "scope": "scope",
            "description": "title description"
        }}
    ]
}}"""

    return prompt

def count_tokens(text: str) -> int:
    """Count tokens using tiktoken if available, otherwise estimate."""
    if HAS_TIKTOKEN:
        try:
            # Use the appropriate model encoding
            encoding = tiktoken.encoding_for_model("gpt-4")
            token_count = len(encoding.encode(text))
            return token_count
        except Exception as e:
            debug_log(f"Tiktoken error: {str(e)}, falling back to estimation", "Warning ‚ö†Ô∏è")
            return len(text) // 4
    else:
        debug_log(
            "Tiktoken not installed. For accurate token counting, install with:\n" +
            "pip install tiktoken",
            "Token Count Info ‚ÑπÔ∏è"
        )
        return len(text) // 4

def get_token_cost(token_count: int) -> str:
    """Calculate cost based on current GPT-4 pricing."""
    # Current GPT-4 Turbo pricing (as of 2024)
    COST_PER_1K_INPUT = 0.01
    COST_PER_1K_OUTPUT = 0.03
    
    # Estimate output tokens as ~25% of input
    estimated_output_tokens = token_count * 0.25
    
    input_cost = (token_count / 1000) * COST_PER_1K_INPUT
    output_cost = (estimated_output_tokens / 1000) * COST_PER_1K_OUTPUT
    total_cost = input_cost + output_cost
    
    return (
        f"Input tokens: {token_count:,}\n"
        f"Estimated output tokens: {int(estimated_output_tokens):,}\n"
        f"Estimated total cost: ${total_cost:.4f}\n"
        f"  - Input cost: ${input_cost:.4f}\n"
        f"  - Output cost: ${output_cost:.4f}"
    )

def get_ai_suggestion(prompt: str, original_message: str) -> Optional[List[Dict[str, str]]]:
    """Get structured commit message suggestions from configured AI provider."""
    config = Config()
    
    # Calculate and log token usage estimation
    token_count = count_tokens(prompt)
    debug_log(
        f"Estimated tokens: {token_count}\n"
        f"Estimated cost: ${(token_count / 1000 * 0.03):.4f} (GPT-4 rate)",
        "Token Usage üí∞"
    )

    if config.get("debug"):
        debug_log(prompt, "AI Prompt")

    # Try Azure OpenAI if available
    if HAS_OPENAI and config.get("ai_provider") == "azure":
        api_key = config.get("azure_api_key") or os.getenv("AZURE_OPENAI_API_KEY")
        if not api_key:
            debug_log("No Azure OpenAI API key found in config or environment", "Warning ‚ö†Ô∏è")
            return None

        try:
            client = AzureOpenAI(
                api_key=api_key,
                api_version=config.get("azure_api_version"),
                azure_endpoint=config.get("azure_endpoint"),
            )
            
            # Default suggestions template
            default_suggestions = [
                {
                    "message": original_message,
                    "explanation": "Original message preserved due to API error",
                    "type": "feat",
                    "scope": "default",
                    "description": original_message
                }
            ]
            
            # Try primary deployment (GPT-4) first
            try:
                debug_log(f"Attempting GPT-4 request to {config.get('azure_deployment')}", "Azure OpenAI ü§ñ")
                
                response = client.chat.completions.create(
                    model=config.get("azure_deployment"),
                    messages=[
                        {
                            "role": "system", 
                            "content": "You are a git commit message assistant. Generate 3 distinct conventional commit format suggestions in JSON format."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1500,
                    n=1,
                    response_format={"type": "json_object"},
                )

                result = json.loads(response.choices[0].message.content)
                suggestions = result.get("suggestions", [])
                
                if suggestions:
                    debug_log(
                        f"Received {len(suggestions)} suggestions\n" + 
                        json.dumps(suggestions, indent=2),
                        "GPT-4 Response ‚ú®"
                    )
                    return suggestions[:3]  # Ensure we return up to 3 suggestions
                return default_suggestions

            except Exception as e:
                debug_log(f"Primary model error: {str(e)}", "Error ‚ùå")
                
                # Try fallback deployment (GPT-3.5) with simplified response
                fallback_deployment = config.get("azure_fallback_deployment")
                if fallback_deployment:
                    try:
                        debug_log(f"Attempting fallback to {fallback_deployment}", "Fallback Model üîÑ")
                        
                        response = client.chat.completions.create(
                            model=fallback_deployment,
                            messages=[
                                {"role": "system", "content": "You are a helpful git commit message assistant."},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.7,
                            max_tokens=500,
                            n=1,
                        )
                        
                        message = response.choices[0].message.content.strip()
                        debug_log(message, "Fallback Response üìù")
                        
                        return [{
                            "message": message,
                            "explanation": "Generated using fallback model",
                            "type": "feat",
                            "scope": "default",
                            "description": message
                        }]
                    except Exception as fallback_error:
                        debug_log(f"Fallback model error: {str(fallback_error)}", "Error ‚ùå")
                        return default_suggestions

        except Exception as e:
            debug_log(f"Azure OpenAI error: {str(e)}", "Error ‚ùå")
            return default_suggestions

    # Fallback to Ollama if configured
    if config.get("ai_provider") == "ollama":
        debug_log("Attempting Ollama request", "Ollama ü§ñ")
        client = OllamaClient(
            host=config.get("ollama_host"),
            model=config.get("ollama_model")
        )
        return client.generate(prompt, original_message)

    return None

def create_commit_message(commit_info: Dict[str, Any], packages: List[Dict[str, Any]]) -> str:
    """Create appropriate commit message based on complexity."""
    # Clean the description to remove any existing type prefix
    description = commit_info['description']
    type_pattern = r'^(feat|fix|docs|style|refactor|test|chore|build|ci|perf|revert)(\([^)]+\))?:\s*'
    if re.match(type_pattern, description):
        description = re.sub(type_pattern, '', description)
        commit_info['description'] = description.strip()

    # Calculate complexity
    complexity = calculate_commit_complexity(packages)
    
    if Config().get("debug"):
        print("\nüîç Commit Complexity Analysis:")
        print(f"Score: {complexity['score']}")
        print("Reasons:")
        for reason in complexity["reasons"]:
            print(f"- {reason}")
    
    # For simple commits, just return the title
    if not complexity["needs_structure"]:
        return f"{commit_info['type']}({commit_info['scope']}): {commit_info['description']}"
    
    # For complex commits, use structured format
    return create_structured_commit(commit_info, packages)

def create_structured_commit(commit_info: Dict[str, Any], packages: List[Dict[str, Any]]) -> str:
    """Create a structured commit message for complex changes."""
    # Clean the description to remove any existing type prefix
    description = commit_info['description']
    type_pattern = r'^(feat|fix|docs|style|refactor|test|chore|build|ci|perf|revert)(\([^)]+\))?:\s*'
    if re.match(type_pattern, description):
        description = re.sub(type_pattern, '', description)

    # Start with the commit title
    message_parts = [
        f"{commit_info['type']}({commit_info['scope']}): {description}"
    ]
    
    # Add a blank line after title
    message_parts.append("")
    
    # Add detailed description if available
    if description := commit_info.get("detailed_description"):
        message_parts.append(description)
        message_parts.append("")
    
    # For multiple packages, add affected packages section
    if len(packages) > 1:
        message_parts.append("Affected packages:")
        for pkg in packages:
            message_parts.append(f"- {pkg['name']}")
            # Add files changed under each package (grouped by type for readability)
            files_by_type = group_files_by_type(pkg["files"])
            for file_type, files in files_by_type.items():
                if files:
                    message_parts.append(f"  {file_type}:")
                    for file in files[:3]:  # Limit to 3 files per type
                        message_parts.append(f"    ‚Ä¢ {file}")
                    if len(files) > 3:
                        message_parts.append(f"    ‚Ä¢ ...and {len(files) - 3} more")
        message_parts.append("")
    
    return "\n".join(message_parts)

def get_package_json_name(package_path: Path) -> Optional[str]:
    """Get package name from package.json if it exists."""
    try:
        pkg_json_path = Path.cwd() / package_path / "package.json"
        if pkg_json_path.exists():
            return json.loads(pkg_json_path.read_text()).get("name")
    except:
        return None
    return None

# ... [previous code remains the same until get_changed_packages]

def get_changed_packages() -> List[Dict]:
    """Get all packages with changes in the current commit."""
    changed_files = check_output(["git", "diff", "--cached", "--name-only"])
    changed_files = changed_files.decode("utf-8").strip().split("\n")

    packages = {}
    for file in changed_files:
        if not file:
            continue

        if file.startswith("packages/"):
            parts = file.split("/")
            if len(parts) > 1:
                pkg_path = f"packages/{parts[1]}"
                if pkg_path not in packages:
                    packages[pkg_path] = []
                packages[pkg_path].append(file)
        else:
            if "root" not in packages:
                packages["root"] = []
            packages["root"].append(file)

    results = []
    for pkg_path, files in packages.items():
        if pkg_path == "root":
            scope = name = "root"
        else:
            pkg_name = get_package_json_name(Path(pkg_path))
            if pkg_name:
                name = pkg_name
                scope = pkg_name.split("/")[-1]
            else:
                name = scope = pkg_path.split("/")[-1]

        results.append({
            "name": name,
            "scope": scope,
            "files": files,
        })

    return results

def get_main_package(packages: List[Dict]) -> Optional[Dict]:
    """Get the package with the most changes."""
    if not packages:
        return {"name": "default", "scope": "default", "files": []}

    # Sort packages by number of files changed
    sorted_packages = sorted(packages, key=lambda x: len(x["files"]), reverse=True)
    return sorted_packages[0]

def prompt_user(question: str) -> bool:
    """Prompt user for yes/no question using /dev/tty."""
    try:
        with open("/dev/tty", "r", encoding="utf-8") as tty:
            print(f"{question} [Y/n]", end=" ", flush=True)
            response = tty.readline().strip().lower()
            return response == "" or response != "n"
    except Exception as e:
        print(f"Warning: Could not get user input: {str(e)}")
        return True

def display_suggestions(suggestions: List[Dict[str, str]]) -> Optional[str]:
    """Display suggestions and get user choice."""
    print("\n‚ú® AI Suggestions:")

    for i, suggestion in enumerate(suggestions, 1):
        print(f"\n{i}. {'=' * 48}")
        print(f"Message: {suggestion['message']}")
        print(f"Type: {suggestion['type']}")
        print(f"Scope: {suggestion['scope']}")
        print(f"Explanation: {suggestion['explanation']}")
        print("=" * 50)

    try:
        with open("/dev/tty", "r", encoding="utf-8") as tty:
            while True:
                print("\nChoose suggestion (1-3) or press Enter to skip: ", end="", flush=True)
                choice = tty.readline().strip()
                if not choice:
                    return None
                if choice in ("1", "2", "3"):
                    return suggestions[int(choice) - 1]["message"]
                print("Please enter 1, 2, 3 or press Enter to skip")
    except EOFError:
        print("\n‚ö†Ô∏è  Input not available. Defaulting to first suggestion.")
        return suggestions[0]["message"] if suggestions else None

def main() -> None:
    """Main function to process git commit messages."""
    try:
        config = Config()
        
        commit_msg_file = sys.argv[1]
        with open(commit_msg_file, "r", encoding="utf-8") as f:
            original_msg = f.read().strip()

        if original_msg.startswith("Merge"):
            sys.exit(0)

        packages = get_changed_packages()
        if not packages:
            sys.exit(0)

        print("\nüîç Analyzing changes...")
        
        # Check if AI is enabled in config
        if config.get("use_ai", True):
            # Generate prompt and calculate cost before asking user
            prompt = enhance_ai_prompt(packages, original_msg)
            token_count = count_tokens(prompt)
            debug_log(
                get_token_cost(token_count),
                "Token Usage üí∞"
            )
            
            # Now ask user if they want to proceed with AI suggestions
            if prompt_user("\nWould you like AI suggestions?"):
                print("\nü§ñ Getting AI suggestions...")
                suggestions = get_ai_suggestion(prompt, original_msg)

                if suggestions:
                    chosen_message = display_suggestions(suggestions)
                    if chosen_message:
                        with open(commit_msg_file, "w", encoding="utf-8") as f:
                            f.write(chosen_message)
                        print("‚úÖ Commit message updated!\n")
                        return

        # Fallback to automatic formatting
        print("\n‚öôÔ∏è Using automatic formatting...")
        commit_info = {
            "type": "feat",  # Default type
            "scope": get_main_package(packages)["scope"],
            "description": original_msg
        }
        new_msg = create_commit_message(commit_info, packages)
        
        print(f"\n‚ú® Suggested message:\n{new_msg}")

        if prompt_user("\nUse suggested message?"):
            with open(commit_msg_file, "w", encoding="utf-8") as f:
                f.write(new_msg)
            print("‚úÖ Commit message updated!\n")

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
